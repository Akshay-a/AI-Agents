import os
import asyncio
import logging
from typing import Dict, Optional, List, Any, Callable ,Awaitable, Tuple
from enum import Enum
import uuid
import json
from models import Task, TaskStatus , JobResultsSummary , FinalReportMessage, TaskType, TaskSuccessMessage, JobFailedMessage
from task_manager import TaskManager # Assuming TaskManager handles status updates/broadcasts
from agents.planning import PlanningAgent # Import agent instances
from agents.search import SearchAgent
from agents.filtering import FilteringAgent
from agents.analysis import AnalysisAgent
from agents.reasoning import ReasoningAgent
from llm_providers import get_provider, BaseLLMProvider

logger = logging.getLogger(__name__)
OUTPUT_DIR = "D:\\Projects\\AI-Agents\\AI-DeepResearch\\DeepResearchAgent\\research_agent_backend\\LLM_outputs\\" 
SEARCH_TASK_DELAY = 1.5

class JobStatus(str, Enum): # This Enum is heart of managing all the jobs and their states
    IDLE = "IDLE"
    RUNNING = "RUNNING"
    PAUSED = "PAUSED" # Paused due to error, waiting for user , in this scenario orchestrator agent handles receving input from user and directs accordingly
    COMPLETED = "COMPLETED"
    FAILED = "FAILED" # Failed definitively

class Orchestrator:
    def __init__(self, task_manager: TaskManager):
        self.task_manager = task_manager
        # Track running/paused jobs. Key: job_id (initial task ID), Value: Status/Event
        self.active_jobs: Dict[str, JobStatus] = {}
        self.pause_events: Dict[str, asyncio.Event] = {}
        self.job_tasks: Dict[str, asyncio.Task] = {} # To manage background tasks -> start with researching about task (that is the first task)

        try:
             # Assuming GOOGLE_API_KEY is set in .env
             self.llm_provider: BaseLLMProvider = get_provider(provider_name="gemini")
             logger.info(f"Initialized LLM Provider: {self.llm_provider.__class__.__name__} with model {self.llm_provider.get_model_name()}")
        except Exception as e:
             logger.error("Failed to initialize LLM Provider. Analysis Agent will fail.")
             # Handle this more gracefully - maybe prevent job start?
             self.llm_provider = None # Ensure it's None if init fails
        self.planning_agent = PlanningAgent(llm_provider=self.llm_provider) if self.llm_provider else None
        self.search_agent = SearchAgent(task_manager=self.task_manager)
        self.filtering_agent = FilteringAgent(task_manager=self.task_manager) 
        
        if self.llm_provider:
             self.analysis_agent = AnalysisAgent(llm_provider=self.llm_provider, task_manager=self.task_manager)
             self.reasoning_agent = ReasoningAgent(llm_provider=self.llm_provider,task_manager=self.task_manager)
        else:
             self.analysis_agent = None # Analysis won't work
             self.reasoning_agent = None

        self.agent_dispatch: Dict[TaskType, Callable[..., Awaitable[None]]] = {}
        if self.reasoning_agent:
            self.agent_dispatch[TaskType.REASON] = self.reasoning_agent.run
        if self.search_agent:
            self.agent_dispatch[TaskType.SEARCH] = self.search_agent.run
        if self.filtering_agent:
            self.agent_dispatch[TaskType.FILTER] = self._run_filter_agent
        if self.analysis_agent:
            self.agent_dispatch[TaskType.SYNTHESIZE] = self._run_analysis_agent 
        # PLAN and REPORT are handled differently (PLAN in start_job, REPORT is often implicit)
        self.agent_dispatch[TaskType.REPORT] = self._run_report_task # Placeholder report task

        logger.info(f"Agent dispatch map configured: {list(self.agent_dispatch.keys())}")
    
    async def start_job(self, user_query: str):
        """Initiates a job by calling the Planning Agent."""
        logger.info(f"Orchestrator: Received start job request for query: '{user_query[:100]}...'")

        if not self.planning_agent:
            logger.error("Cannot start job: Planning Agent not initialized (LLM provider likely failed).")
            # Notify user via websocket?
            raise RuntimeError("Planning Agent not available.")

        # Use a temporary ID for logging before plan is made
        temp_job_id = f"planning-{uuid.uuid4()}"
        initial_task_id = None
        job_id = None

        try:
            # 1. Generate Plan
            # The plan is now a list of dicts with 'task_type', 'description', 'parameters'
            planned_tasks_data = await self.planning_agent.generate_plan(user_query, temp_job_id)

            if not planned_tasks_data:
                logger.error(f"Planning Agent returned an empty plan for query: {user_query}")
                raise ValueError("Planning failed: No tasks generated.")

            # 2. Add Tasks to TaskManager
            tasks_in_job: List[Task] = []
            first_task = True
            for task_data in planned_tasks_data:
                # Use the ID of the *first* task generated by the planner as the Job ID
                if first_task:
                    # Create the first task to establish the job ID
                    new_task = await self.task_manager.add_task(
                        description=task_data["description"],
                        task_type=task_data["task_type"],
                        parameters=task_data["parameters"],
                        job_id=None # Set job_id after creation
                    )
                    job_id = new_task.id # Assign Job ID based on the first task
                    new_task.job_id = job_id # Update the task object
                    self.task_manager.tasks[job_id] = new_task # Ensure update in manager dict
                    logger.info(f"Established Job ID {job_id} from first planned task.")
                    initial_task_id = job_id
                    tasks_in_job.append(new_task)
                    first_task = False
                else:
                    # Add subsequent tasks with the established job ID
                    new_task = await self.task_manager.add_task(
                        description=task_data["description"],
                        task_type=task_data["task_type"],
                        parameters=task_data["parameters"],
                        job_id=job_id # Use the established job ID
                    )
                    tasks_in_job.append(new_task)

            # Ensure all tasks are saved with the correct job ID
            #save_tasks_to_md(self.task_manager.tasks)

            # 3. Start Orchestrator Background Task
            if job_id and initial_task_id: # Check if job was successfully created
                 if job_id in self.active_jobs:   
                     logger.warning(f"Job {job_id} is already active (potential race condition?).")
                     return job_id, planned_tasks_data # Return existing job ID and plan data

                 logger.info(f"Starting background execution for job {job_id}...")
                 self.active_jobs[job_id] = JobStatus.RUNNING
                 self.pause_events[job_id] = asyncio.Event() # Initially unset

                 job_task = asyncio.create_task(
                     self._run_research_flow(job_id),
                     name=f"Job-{job_id}"
                 )
                 self.job_tasks[job_id] = job_task
                 await self.broadcast_job_status(job_id, JobStatus.RUNNING, f"Starting research on: {user_query[:50]}...")
                 return job_id, planned_tasks_data # Return the job ID and plan data
            else:
                 logger.error("Failed to create initial task and establish Job ID.")
                 raise RuntimeError("Failed to initialize job tasks.")

        except Exception as e:
            logger.exception(f"Error during job planning/initialization for query '{user_query[:100]}...': {e}")
            # Optionally broadcast an error to the user?
            await self.task_manager.connection_manager.broadcast_json({
                "type": "job_error",
                "detail": f"Failed to plan job for query '{user_query[:50]}...': {e}"
            })
            # Re-raise or return an error indicator
            raise # Let the calling endpoint handle the HTTP error
        
    async def start_job_old(self, initial_task_id: str, topic: str):
        """Initiates and runs the research flow for a given job"""
        
        if initial_task_id in self.active_jobs:
            logger.warning(f"Job {initial_task_id} is already active.")
            return

        logger.info(f"Starting job {initial_task_id} for topic: {topic}")
        self.active_jobs[initial_task_id] = JobStatus.RUNNING
        self.pause_events[initial_task_id] = asyncio.Event() # Initially unset (not paused)

        # Run the main flow in a background task
        job_task = asyncio.create_task(
            self._run_research_flow(initial_task_id),
            name=f"Job-{initial_task_id}"
        )
        self.job_tasks[initial_task_id] = job_task
        await self.broadcast_job_status(initial_task_id, JobStatus.RUNNING, f"Job started for topic: {topic}")


    async def _run_research_flow(self, job_id: str):
        """
        Main background execution loop for a single research job.
        Fetches pending tasks, dispatches them, handles state, errors, and completion.
        """
        job_final_status = JobStatus.FAILED # Default unless explicitly completed
        final_report_content = None
        
        try:
            logger.info(f"Job {job_id}: Workflow execution started.")
            
            # Track the current phase of research to broadcast appropriate high-level messages
            current_phase = None
            search_phase_started = False
            filter_phase_started = False
            analysis_phase_started = False
            
            while self.active_jobs.get(job_id) == JobStatus.RUNNING:
                # --- Pause Check ---
                if self.pause_events[job_id].is_set():
                    logger.info(f"Job {job_id} is paused. Waiting for resume signal...")
                    # Wait until the event is cleared (by resume command)
                    if self.active_jobs.get(job_id) != JobStatus.RUNNING:
                         logger.info(f"Job {job_id} resume detected, but status is now {self.active_jobs.get(job_id)}. Exiting loop.")
                         break
                    logger.info(f"Job {job_id} resumed.")
                    # Continue to next loop iteration after resume
                    continue

                # --- Get Next Pending Task ---
                next_task = self.task_manager.get_next_pending_task_for_job(job_id)

                # --- Check for Job Completion or Waiting State ---
                if not next_task:
                    if self.task_manager.has_running_tasks(job_id):
                        # If no pending tasks but some are still running, wait
                        logger.info(f"Job {job_id}: No pending tasks, but waiting for {len([t for t in self.task_manager.tasks.values() if t.job_id == job_id and t.status == TaskStatus.RUNNING])} running tasks.")
                        await asyncio.sleep(1) # Wait a bit before checking again
                        continue
                    else:
                        # No pending and no running tasks left
                        logger.info(f"Job {job_id}: No more pending or running tasks.")
                        if not self.task_manager.has_errored_tasks(job_id):
                             logger.info(f"Job {job_id} completed successfully.")
                             job_final_status = JobStatus.COMPLETED
                             await self.broadcast_job_status(job_id, JobStatus.COMPLETED, "Research completed successfully!")
                        else:
                             logger.warning(f"Job {job_id} finished, but with errors or skipped tasks.")
                             job_final_status = JobStatus.FAILED
                             # Send a user-friendly failure message
                             await self.task_manager.connection_manager.broadcast_json(
                                 JobFailedMessage(
                                     job_id=job_id
                                 ).dict()
                             )
                             break # Exit without trying to generate a report

                        # --- Final Report Generation (only if job was successful) ---
                        if job_final_status == JobStatus.COMPLETED:
                            logger.info(f"Job {job_id} finished successfully. Generating final report...")
                            final_report_content = await self._handle_final_report(job_id)
                            
                            if not final_report_content:
                                logger.warning(f"Job {job_id}: No valid report content found. Marking as failed.")
                                job_final_status = JobStatus.FAILED
                                await self.task_manager.connection_manager.broadcast_json(
                                    JobFailedMessage(
                                        job_id=job_id,
                                        message="Failed to generate a report. Please try again or rephrase your query."
                                    ).dict()
                                )
                        break # Exit the while loop, job is done

                # --- Broadcast Phase Updates ---
                # Check if we're entering a new phase based on next task type
                if next_task.task_type == TaskType.SEARCH and not search_phase_started:
                    await self.broadcast_job_status(job_id, JobStatus.RUNNING, "Researching sources...")
                    search_phase_started = True
                elif next_task.task_type == TaskType.FILTER and not filter_phase_started:
                    await self.broadcast_job_status(job_id, JobStatus.RUNNING, "Consolidating information...")
                    filter_phase_started = True
                elif (next_task.task_type == TaskType.SYNTHESIZE or next_task.task_type == TaskType.REASON) and not analysis_phase_started:
                    await self.broadcast_job_status(job_id, JobStatus.RUNNING, "Analyzing and synthesizing...")
                    analysis_phase_started = True

                # --- Execute the Next Task ---
                await self.task_manager.update_task_status(next_task.id, TaskStatus.RUNNING, detail="Orchestrator picked up task.")
                task_failed = False
                try:
                    # Dispatch task to the appropriate agent via the dispatch map
                    await self._dispatch_task(next_task)

                    # If _dispatch_task completes *without raising an exception*,
                    # check the task's status. If it's still RUNNING, mark COMPLETED.
                    current_task_state = self.task_manager.get_task(next_task.id)
                    if current_task_state and current_task_state.status == TaskStatus.RUNNING:
                        await self.task_manager.update_task_status(
                            next_task.id,
                            TaskStatus.COMPLETED,
                            detail="Task completed successfully by agent."
                        )
                        
                        # Broadcast task success message (but not full task update)
                        await self.task_manager.connection_manager.broadcast_json(
                            TaskSuccessMessage(
                                job_id=job_id,
                                task_id=next_task.id,
                                task_type=next_task.task_type
                            ).dict()
                        )
                    elif current_task_state:
                         logger.debug(f"Task {next_task.id} finished with status {current_task_state.status} (not RUNNING). Not marking completed by Orchestrator.")
                    else:
                         # Should not happen if task existed before dispatch
                         logger.warning(f"Task {next_task.id} not found after dispatch. Cannot update status.")

                except Exception as e:
                    # Handle errors *raised by the agent execution* (_dispatch_task re-raises them)
                    task_failed = True
                    logger.error(f"Error executing Task {next_task.id} (Type: {next_task.task_type.value}): {e}", exc_info=False) # Log simply here
                    error_msg = str(e)
                    # Ensure task exists before updating status to ERROR
                    if self.task_manager.get_task(next_task.id):
                        await self.task_manager.update_task_status(
                            next_task.id,
                            TaskStatus.ERROR,
                            error_message=error_msg,
                            detail=f"Task failed during execution: {error_msg[:100]}..."
                        )
                    else:
                        logger.error(f"Task {next_task.id} not found, cannot mark as ERROR after execution failure.")

                # --- Loop Delay ---
                # Add a small delay unless a task just failed (to avoid immediate re-check after pause)
                if not task_failed:
                    await asyncio.sleep(0.1)
        except asyncio.CancelledError:
             logger.info(f"Job {job_id} task was cancelled.")
             job_final_status = JobStatus.FAILED
             await self.task_manager.connection_manager.broadcast_json(
                 JobFailedMessage(
                     job_id=job_id,
                     message="Research was cancelled before completion."
                 ).dict()
             )
        except Exception as e:
            logger.exception(f"Critical unexpected error in research flow for job {job_id}: {e}")
            job_final_status = JobStatus.FAILED
            await self.task_manager.connection_manager.broadcast_json(
                 JobFailedMessage(
                     job_id=job_id,
                     message="An unexpected error occurred. Please try again."
                 ).dict()
             )
        finally:
            # Final actions regardless of how the loop exited
            self.active_jobs[job_id] = job_final_status # Update final status
            logger.info(f"Exiting research flow loop for job {job_id}. Final Status: {job_final_status.value}")
            # Clean up resources for this job
            self.pause_events.pop(job_id, None)
            self.job_tasks.pop(job_id, None)
            logger.info(f"Cleaned up resources for job {job_id}.")

    async def _dispatch_task(self, task: Task):
        """
        Looks up the appropriate agent method based on TaskType and executes it,
        passing necessary parameters. Handles cases where agents are unavailable.
        Raises exceptions if agent execution fails.
        """
        logger.info(f"Dispatching Task Type: {task.task_type.value}, ID: {task.id}, Job: {task.job_id}")
        logger.info(f"Task parameters: {json.dumps(task.parameters)}")

        agent_method = self.agent_dispatch.get(task.task_type)

        if not agent_method:
            # Handle PLAN tasks (shouldn't be dispatched by loop) and unknown types
            if task.task_type == TaskType.PLAN:
                 logger.warning(f"PLAN task {task.id} reached dispatch logic unexpectedly. Marking completed.")
                 await self.task_manager.update_task_status(task.id, TaskStatus.COMPLETED, detail="Planning completed (conceptual task).")
                 return # Don't dispatch
            else:
                 logger.error(f"No agent method configured or agent unavailable for task type: {task.task_type.value}. Skipping task {task.id}.")
                 await self.task_manager.update_task_status(task.id, TaskStatus.SKIPPED, detail=f"No handler for task type {task.task_type.value}")
                 return # Skip if no handler

        # Prepare arguments for the agent method
        # Standard args: task_id, job_id
        # Specific args come from task.parameters
        kwargs_for_agent = task.parameters.copy() # Start with parameters dict
        kwargs_for_agent['task_id'] = task.id
        kwargs_for_agent['job_id'] = task.job_id

        try:
            # Call the appropriate agent method (e.g., self.search_agent.run(**kwargs_for_agent))
            # The agent method is responsible for its own logic and storing results via TaskManager
            await agent_method(**kwargs_for_agent)
            logger.info(f"Agent method for task {task.id} (Type: {task.task_type.value}) completed execution.")
        except Exception as agent_exec_error:
             # Log the specific error from the agent call
             logger.error(f"Agent execution failed for task {task.id} (Type: {task.task_type.value}). Error: {agent_exec_error}", exc_info=True)
             # Re-raise the exception so the main loop (_run_research_flow) catches it,
             # logs it again (with less detail maybe), sets task status to ERROR, and pauses the job.
             raise agent_exec_error
    async def _run_filter_agent(self, task_id: str, job_id: str, **kwargs):
         """Wrapper to find inputs (previous search task IDs) for the filter agent."""
         logger.info(f"Filter wrapper called for task {task_id}, job {job_id}. Kwargs ignored: {kwargs}")
         # Filter agent needs IDs of preceding *completed* SEARCH tasks
        
         # Get completed search tasks for this job  
         search_tasks = self.task_manager.get_completed_tasks_for_job(job_id, task_type=TaskType.SEARCH)
         search_task_ids = [st.id for st in search_tasks]
         
         if not search_task_ids:
              logger.warning(f"[Job {job_id} | Task {task_id}] Filter task found no preceding completed Search tasks.")
         else:
              logger.info(f"[Job {job_id} | Task {task_id}] Filter task using results from {len(search_tasks)} completed search tasks with IDs: {search_task_ids}")

         # Call the actual filter agent run method
         await self.filtering_agent.run(search_task_ids=search_task_ids, current_task_id=task_id, job_id=job_id)

         # Broadcast summary after filter completes successfully 
         filter_result = self.task_manager.get_result(task_id)
         if filter_result and isinstance(filter_result, dict):
             filtered_items = filter_result.get("filtered_results", [])
             summary = JobResultsSummary(
                     job_id=job_id,
                     unique_sources_found=len(filtered_items),
                     duplicates_removed=filter_result.get("duplicates_removed", 0),
                     sources_analyzed_count=filter_result.get("sources_analyzed_count", len(search_tasks)),
                     top_sources=[
                         {"title": item.get("title", "N/A"), "url": item.get("url", "N/A")}
                         for item in filtered_items[:5] if isinstance(item, dict)
                     ]
                 )
             await self.broadcast_job_summary(summary)


    async def _run_analysis_agent(self, task_id: str, job_id: str, topic: str, **kwargs):
         """Wrapper to find inputs (preceding filter task ID) for the analysis agent."""
         logger.debug(f"Analysis wrapper called for task {task_id}, job {job_id}. Topic: {topic}. Kwargs ignored: {kwargs}")
         # Analysis agent needs the filter results from the preceding filter task
         
         # Get the completed filter tasks for this job
         filter_tasks = self.task_manager.get_completed_tasks_for_job(job_id, task_type=TaskType.FILTER)
         
         if not filter_tasks:
             logger.error(f"[Job {job_id} | Task {task_id}] Cannot run Analysis: No preceding completed Filter task found.")
             # Raise error to stop processing this task
             raise ValueError("Preceding Filter task not found or not completed for Analysis.")
         
         # Assume latest completed filter task is the relevant one
         filter_task = filter_tasks[-1]  # Last one is most recent
         filter_task_id = filter_task.id
         
         # Get the actual filter result
         filter_result = self.task_manager.get_result(filter_task_id)
         if not filter_result:
             logger.error(f"[Job {job_id} | Task {task_id}] Filter task {filter_task_id} has no stored result.")
             raise ValueError(f"Filter task {filter_task_id} has no result for Analysis.")
             
         logger.info(f"[Job {job_id} | Task {task_id}] Analysis task using results from Filter Task {filter_task_id}")

         # Call the actual analysis agent run method with the filter result
         await self.analysis_agent.run(
             topic=topic, # Passed from parameters via kwargs_for_agent -> **kwargs
             filter_result=filter_result,  # Pass filter result directly
             current_task_id=task_id,
             job_id=job_id
         )


    async def _run_report_task(self, task_id: str, job_id: str, source_task_id: Optional[str] = None, **kwargs):
         """
         Handles the REPORT task. Typically finds the result of the preceding
         SYNTHESIZE or REASON task and stores it under the REPORT task's ID.
         If no such task is available, will try to use filter results directly.
         """
         logger.debug(f"Report wrapper called for task {task_id}, job {job_id}. Source: {source_task_id}. Kwargs ignored: {kwargs}")
         report_content = None
         
         if source_task_id:
             # Use specified source if provided
             result_to_report = self.task_manager.get_result(source_task_id)
             if result_to_report is None:
                 logger.warning(f"Report task {task_id} could not find result for specified source task {source_task_id}.")
                 # Continue to fallback methods
             else:
                 # Extract content from the specified source
                 if isinstance(result_to_report, dict):
                     report_content = result_to_report.get("report") or result_to_report.get("analysis_output") or result_to_report.get("reasoning_output")
                 elif isinstance(result_to_report, str):
                     report_content = result_to_report # Assume direct string output is the report
         
         # If no report content yet, try to find it from completed tasks
         if not report_content:
             # Find latest completed synthesis or reasoning task
             # Get all task objects from task_manager (not results!)
             tasks = list(self.task_manager.tasks.values())
             synthesis_tasks = []
             for task in reversed(tasks):
                 if task.job_id == job_id and task.status == TaskStatus.COMPLETED:
                     if task.task_type in [TaskType.SYNTHESIZE, TaskType.REASON]:
                         synthesis_tasks.append(task)
                         
             if synthesis_tasks:
                 # Use the latest completed task
                 latest_task = synthesis_tasks[0]
                 source_task_id = latest_task.id
                 logger.info(f"Report task {task_id} automatically targeting result from task {source_task_id} (Type: {latest_task.task_type.value})")
                 result_to_report = self.task_manager.get_result(source_task_id)
                 
                 if result_to_report:
                     # Extract content
                     if isinstance(result_to_report, dict):
                         report_content = result_to_report.get("report") or result_to_report.get("analysis_output") or result_to_report.get("reasoning_output")
                     elif isinstance(result_to_report, str):
                         report_content = result_to_report
             
         # Fallback: if still no report content, try using filter results directly
         if not report_content:
             logger.warning(f"Report task {task_id} could not find a relevant source task. Trying to use filter results as fallback.")
             
             # Get the latest filter task results
             filter_tasks = [t for t in self.task_manager.tasks.values() 
                            if t.job_id == job_id and t.status == TaskStatus.COMPLETED and t.task_type == TaskType.FILTER]
             
             if filter_tasks:
                 filter_task = filter_tasks[-1]  # Use the latest one
                 filter_result = self.task_manager.get_result(filter_task.id)
                 
                 if filter_result and isinstance(filter_result, dict) and "filtered_results" in filter_result:
                     # Generate a simple report from filter results
                     filtered_items = filter_result.get("filtered_results", [])
                     if filtered_items:
                         logger.info(f"Generating a simple report from {len(filtered_items)} filtered items as fallback.")
                         
                         # Create a simple markdown report listing the sources
                         report_lines = [
                             "# AI Impact on Software Development - Information Sources",
                             "",
                             "The following sources contain information about the impact of AI on software development jobs:",
                             ""
                         ]
                         
                         # Add the sources
                         for i, item in enumerate(filtered_items[:10], 1):  # Top 10 sources
                             title = item.get("title", "Untitled Source")
                             url = item.get("url", "No URL")
                             report_lines.append(f"{i}. [{title}]({url})")
                         
                         report_lines.extend([
                             "",
                             "## Note",
                             "",
                             "This is a simplified report listing sources only. The detailed synthesis could not be completed successfully."
                         ])
                         
                         report_content = "\n".join(report_lines)
         
         # If we have report content, store it
         if report_content and isinstance(report_content, str):
             # Store the extracted or generated content
             await self.task_manager.store_result(task_id, {"report": report_content})
             logger.info(f"Report task {task_id} completed with content from {source_task_id if source_task_id else 'fallback mechanism'}.")
             return
         
         # If we reached here, we couldn't generate any report
         logger.warning(f"Report task {task_id}: Could not extract or generate any report content.")
         await self.task_manager.store_result(task_id, {"report": "No report could be generated due to missing or invalid source data."})
         # Don't raise an exception - just store a placeholder report


    # --- Final Report Handling (Called at end of job) ---
    async def _handle_final_report(self, job_id: str):
        """
        Finds the final report and returns it.
        If found, saves and broadcasts the report.
        Returns the report content on success, None on failure.
        """
        logger.info(f"Job {job_id}: Looking for final report content...")
        
        # First try to find completed REPORT tasks
        report_task = None
        synthesis_task = None
        reason_task = None
        
        # Get all tasks from task_manager (not results!)
        tasks = list(self.task_manager.tasks.values())
        
        # Check tasks in reverse order for the most recent relevant one
        for task in reversed(tasks):
            if task.job_id == job_id and task.status == TaskStatus.COMPLETED:
                if task.task_type == TaskType.REPORT and not report_task:
                    report_task = task
                elif task.task_type == TaskType.SYNTHESIZE and not synthesis_task:
                    synthesis_task = task
                elif task.task_type == TaskType.REASON and not reason_task:
                    reason_task = task

        # Use the highest priority task found (REPORT > SYNTHESIZE > REASON)
        last_relevant_task = report_task or synthesis_task or reason_task

        if last_relevant_task:
             final_result = self.task_manager.get_result(last_relevant_task.id)
             report_markdown = None
             if isinstance(final_result, dict):
                 # Look for common keys
                 report_markdown = final_result.get("report") or final_result.get("analysis_output") or final_result.get("reasoning_output")
             elif isinstance(final_result, str):
                 report_markdown = final_result # Simple string result

             if isinstance(report_markdown, str) and report_markdown.strip():
                 logger.info(f"Found final report content in task {last_relevant_task.id} (Type: {last_relevant_task.task_type.value}). Saving and broadcasting.")
                 await self._save_report_to_file(job_id, report_markdown)
                 await self._broadcast_final_report(job_id, report_markdown)
                 return report_markdown
             else:
                 logger.warning(f"Job {job_id} finished, but final report from task {last_relevant_task.id} was empty or invalid.")
                 return None
        else:
             logger.warning(f"Job {job_id} finished, but no final REPORT, SYNTHESIZE, or REASON task found with results.")
             return None

    async def resume(self, job_id: str):
        """Resumes a paused job."""
        if job_id in self.pause_events and self.pause_events[job_id].is_set():
             logger.info(f"Received resume command for paused job {job_id}.")
             self.active_jobs[job_id] = JobStatus.RUNNING # Ensure status is RUNNING
             self.pause_events[job_id].clear() # Clear the event to allow the loop to continue
             # Don't broadcast here, the loop will broadcast upon resuming
        else:
             logger.warning(f"Received resume command for job {job_id}, but it was not paused or doesn't exist.")

    async def skip_task_and_resume(self, job_id: str, task_id: str):
         """Marks a task as skipped and potentially resumes the job."""
         logger.info(f"Received skip command for task {task_id} in job {job_id}.")
         await self.task_manager.update_task_status(task_id, TaskStatus.SKIPPED, detail="Task skipped by user.")
         # If the job was paused specifically because *this task* failed, resume it.
         if job_id in self.pause_events and self.pause_events[job_id].is_set():
              # Check if the paused state was due to this task (may need better state tracking)
              # Simple assumption: if paused, skipping an error task should allow resume
              logger.info(f"Job {job_id} was paused, resuming after skipping task {task_id}.")
              await self.resume(job_id)


    async def broadcast_job_status(self, job_id: str, status: JobStatus, detail: str):
        """Broadcasts overall job status updates via WebSocket."""
        message = {
            "type": "job_status",
            "job_id": job_id,
            "status": status.value,
            "detail": detail
        }
        await self.task_manager.connection_manager.broadcast_json(message) # Use TM's connection mgr

    async def broadcast_job_summary(self, summary: JobResultsSummary):
        """Broadcasts the research results summary via WebSocket."""
        logger.info(f"Broadcasting results summary for Job {summary.job_id}: Found {summary.unique_sources_found} sources.")
        # Use TaskManager's connection_manager instance
        await self.task_manager.connection_manager.broadcast_json(summary.dict())

    async def _save_report_to_file(self, job_id: str, report_markdown: str):
        """Saves the final report markdown to a file."""
        try:
            # Create output directory if it doesn't exist
            os.makedirs(OUTPUT_DIR, exist_ok=True)

            # Sanitize job_id for filename (optional, depends on job_id format)
            safe_job_id = job_id.replace(":", "_").replace("/", "_").replace("\\", "_")
            filename = os.path.join(OUTPUT_DIR, f"{safe_job_id}_report.md")

            with open(filename, "w", encoding="utf-8") as f:
                f.write(report_markdown)
            logger.info(f"Successfully saved report for job {job_id} to {filename}")

        except IOError as e:
            logger.error(f"Failed to save report file for job {job_id}: {e}")
        except Exception as e:
            logger.exception(f"An unexpected error occurred while saving report for job {job_id}: {e}")

    async def _broadcast_final_report(self, job_id: str, report_markdown: str):
        """Broadcasts the final report via WebSocket."""
        logger.info(f"Broadcasting final report for job {job_id} via WebSocket.")
        message = FinalReportMessage(job_id=job_id, report_markdown=report_markdown)
        # Use TaskManager's connection_manager instance
        await self.task_manager.connection_manager.broadcast_json(message.dict())
    


    async def broadcast_job_summary(self, summary: JobResultsSummary):
         """Broadcasts the research results summary via WebSocket."""
         logger.info(f"Broadcasting results summary for Job {summary.job_id}: Found {summary.unique_sources_found} sources.")
         await self.task_manager.connection_manager.broadcast_json(summary.dict())

    async def _handle_report_completion(self, task: Task):
        """
        Checks if a completed task is the final report/analysis task,
        and if so, retrieves, saves, and broadcasts the report.
        """
        desc_lower = task.description.lower()
        job_id = task.job_id # Get job_id from the task object

        # Check if this task type is expected to produce the final report
        if "analyze" in desc_lower or "synthesize" in desc_lower or "generate report" in desc_lower:
            logger.info(f"[Job {job_id}] Task {task.id} ({task.description}) completed. Checking for final report...")

            analysis_result = self.task_manager.get_result(task.id)
            logger.debug(f"[Job {job_id}] Result retrieved for task {task.id}: {analysis_result is not None}") # Log if result exists

            if analysis_result and isinstance(analysis_result, dict) and "report" in analysis_result:
                report_markdown = analysis_result["report"]
                if isinstance(report_markdown, str) and report_markdown.strip():
                    logger.info(f"[Job {job_id}] Valid report found for task {task.id}. Saving and broadcasting.")
                    # Perform saving and broadcasting
                    await self._save_report_to_file(job_id, report_markdown)
                    await self._broadcast_final_report(job_id, report_markdown)
                else:
                    logger.warning(f"[Job {job_id}] Report found for task {task.id}, but it's empty or not a string.")
            else:
                logger.warning(f"[Job {job_id}] Analysis/Report task {task.id} completed, but no valid 'report' key found in its result.")
        # else:
            # logger.debug(f"Task {task.id} ({task.description}) is not a report-generating task. Skipping report handling.")
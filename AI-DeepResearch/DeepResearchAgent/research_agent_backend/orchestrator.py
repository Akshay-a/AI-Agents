import os
import asyncio
import logging
from typing import Dict, Optional, List, Any, Callable ,Awaitable
from enum import Enum
import uuid
import json
from models import Task, TaskStatus , JobResultsSummary , FinalReportMessage, TaskType
from task_manager import TaskManager # Assuming TaskManager handles status updates/broadcasts
from agents.planning import PlanningAgent # Import agent instances
from agents.search import SearchAgent
from agents.filtering import FilteringAgent
from agents.analysis import AnalysisAgent
from agents.reasoning import ReasoningAgent
from llm_providers import get_provider, BaseLLMProvider

logger = logging.getLogger(__name__)
OUTPUT_DIR = "D:\\Projects\\AI-Agents\\AI-DeepResearch\\DeepResearchAgent\\research_agent_backend\\LLM_outputs\\" 
SEARCH_TASK_DELAY = 1.5

class JobStatus(str, Enum): # This Enum is heart of managing all the jobs and their states
    IDLE = "IDLE"
    RUNNING = "RUNNING"
    PAUSED = "PAUSED" # Paused due to error, waiting for user , in this scenario orchestrator agent handles receving input from user and directs accordingly
    COMPLETED = "COMPLETED"
    FAILED = "FAILED" # Failed definitively

class Orchestrator:
    def __init__(self, task_manager: TaskManager):
        self.task_manager = task_manager
        # Track running/paused jobs. Key: job_id (initial task ID), Value: Status/Event
        self.active_jobs: Dict[str, JobStatus] = {}
        self.pause_events: Dict[str, asyncio.Event] = {}
        self.job_tasks: Dict[str, asyncio.Task] = {} # To manage background tasks -> start with researching about task (that is the first task)

        try:
             # Assuming GOOGLE_API_KEY is set in .env
             self.llm_provider: BaseLLMProvider = get_provider(provider_name="gemini")
             logger.info(f"Initialized LLM Provider: {self.llm_provider.__class__.__name__} with model {self.llm_provider.get_model_name()}")
        except Exception as e:
             logger.error("Failed to initialize LLM Provider. Analysis Agent will fail.")
             # Handle this more gracefully - maybe prevent job start?
             self.llm_provider = None # Ensure it's None if init fails
        self.planning_agent = PlanningAgent(llm_provider=self.llm_provider) if self.llm_provider else None
        self.search_agent = SearchAgent(task_manager=self.task_manager)
        self.filtering_agent = FilteringAgent(task_manager=self.task_manager) 
        
        if self.llm_provider:
             self.analysis_agent = AnalysisAgent(llm_provider=self.llm_provider, task_manager=self.task_manager)
             self.reasoning_agent = ReasoningAgent(llm_provider=self.llm_provider,task_manager=self.task_manager)
        else:
             self.analysis_agent = None # Analysis won't work
             self.reasoning_agent = None

        self.agent_dispatch: Dict[TaskType, Callable[..., Awaitable[None]]] = {}
        if self.reasoning_agent:
            self.agent_dispatch[TaskType.REASON] = self.reasoning_agent.run
        if self.search_agent:
            self.agent_dispatch[TaskType.SEARCH] = self.search_agent.run
        if self.filtering_agent:
            self.agent_dispatch[TaskType.FILTER] = self._run_filter_agent
        if self.analysis_agent:
            self.agent_dispatch[TaskType.SYNTHESIZE] = self._run_analysis_agent 
        # PLAN and REPORT are handled differently (PLAN in start_job, REPORT is often implicit)
        self.agent_dispatch[TaskType.REPORT] = self._run_report_task # Placeholder report task

        logger.info(f"Agent dispatch map configured: {list(self.agent_dispatch.keys())}")
    
    async def start_job(self, user_query: str):
        """Initiates a job by calling the Planning Agent."""
        logger.info(f"Orchestrator: Received start job request for query: '{user_query[:100]}...'")

        if not self.planning_agent:
            logger.error("Cannot start job: Planning Agent not initialized (LLM provider likely failed).")
            # Notify user via websocket?
            raise RuntimeError("Planning Agent not available.")

        # Use a temporary ID for logging before plan is made
        temp_job_id = f"planning-{uuid.uuid4()}"
        initial_task_id = None
        job_id = None

        try:
            # 1. Generate Plan
            # The plan is now a list of dicts with 'task_type', 'description', 'parameters'
            planned_tasks_data = await self.planning_agent.generate_plan(user_query, temp_job_id)

            if not planned_tasks_data:
                logger.error(f"Planning Agent returned an empty plan for query: {user_query}")
                raise ValueError("Planning failed: No tasks generated.")

            # 2. Add Tasks to TaskManager
            tasks_in_job: List[Task] = []
            first_task = True
            for task_data in planned_tasks_data:
                # Use the ID of the *first* task generated by the planner as the Job ID
                if first_task:
                    # Create the first task to establish the job ID
                    new_task = await self.task_manager.add_task(
                        description=task_data["description"],
                        task_type=task_data["task_type"],
                        parameters=task_data["parameters"],
                        job_id=None # Set job_id after creation
                    )
                    job_id = new_task.id # Assign Job ID based on the first task
                    new_task.job_id = job_id # Update the task object
                    self.task_manager.tasks[job_id] = new_task # Ensure update in manager dict
                    logger.info(f"Established Job ID {job_id} from first planned task.")
                    initial_task_id = job_id
                    tasks_in_job.append(new_task)
                    first_task = False
                else:
                    # Add subsequent tasks with the established job ID
                    new_task = await self.task_manager.add_task(
                        description=task_data["description"],
                        task_type=task_data["task_type"],
                        parameters=task_data["parameters"],
                        job_id=job_id # Use the established job ID
                    )
                    tasks_in_job.append(new_task)

            # Ensure all tasks are saved with the correct job ID
            #save_tasks_to_md(self.task_manager.tasks)

            # 3. Start Orchestrator Background Task
            if job_id and initial_task_id: # Check if job was successfully created
                 if job_id in self.active_jobs:   
                     logger.warning(f"Job {job_id} is already active (potential race condition?).")
                     return job_id # Return existing job ID

                 logger.info(f"Starting background execution for job {job_id}...")
                 self.active_jobs[job_id] = JobStatus.RUNNING
                 self.pause_events[job_id] = asyncio.Event() # Initially unset

                 job_task = asyncio.create_task(
                     self._run_research_flow(job_id),
                     name=f"Job-{job_id}"
                 )
                 self.job_tasks[job_id] = job_task
                 await self.broadcast_job_status(job_id, JobStatus.RUNNING, f"Job started for query: {user_query[:50]}...")
                 return job_id # Return the actual job ID
            else:
                 logger.error("Failed to create initial task and establish Job ID.")
                 raise RuntimeError("Failed to initialize job tasks.")

        except Exception as e:
            logger.exception(f"Error during job planning/initialization for query '{user_query[:100]}...': {e}")
            # Optionally broadcast an error to the user?
            await self.task_manager.connection_manager.broadcast_json({
                "type": "job_error",
                "detail": f"Failed to plan job for query '{user_query[:50]}...': {e}"
            })
            # Re-raise or return an error indicator
            raise # Let the calling endpoint handle the HTTP error
        
    async def start_job_old(self, initial_task_id: str, topic: str):
        """Initiates and runs the research flow for a given job"""
        
        if initial_task_id in self.active_jobs:
            logger.warning(f"Job {initial_task_id} is already active.")
            return

        logger.info(f"Starting job {initial_task_id} for topic: {topic}")
        self.active_jobs[initial_task_id] = JobStatus.RUNNING
        self.pause_events[initial_task_id] = asyncio.Event() # Initially unset (not paused)

        # Run the main flow in a background task
        job_task = asyncio.create_task(
            self._run_research_flow(initial_task_id),
            name=f"Job-{initial_task_id}"
        )
        self.job_tasks[initial_task_id] = job_task
        await self.broadcast_job_status(initial_task_id, JobStatus.RUNNING, f"Job started for topic: {topic}")


    async def _run_research_flow(self, job_id: str):
        """
        Main background execution loop for a single research job.
        Fetches pending tasks, dispatches them, handles state, errors, and completion.
        """
        job_final_status = JobStatus.FAILED # Default unless explicitly completed
        try:
            logger.info(f"Job {job_id}: Workflow execution started.")
            while self.active_jobs.get(job_id) == JobStatus.RUNNING:
                # --- Pause Check ---
                if self.pause_events[job_id].is_set():
                    logger.info(f"Job {job_id} is paused. Waiting for resume signal...")
                    #await self.broadcast_job_status(job_id, JobStatus.PAUSED, "Paused due to error or input required. Waiting for user.")
                    # Wait until the event is cleared (by resume command)
                    #await self.pause_events[job_id].wait()
                    # Check if job status changed while paused (e.g., cancelled)
                    if self.active_jobs.get(job_id) != JobStatus.RUNNING:
                         logger.info(f"Job {job_id} resume detected, but status is now {self.active_jobs.get(job_id)}. Exiting loop.")
                         break
                    logger.info(f"Job {job_id} resumed.")
                    #await self.broadcast_job_status(job_id, JobStatus.RUNNING, "Job resumed by user.")
                    # Continue to next loop iteration after resume
                    continue

                # --- Get Next Pending Task ---
                next_task = self.task_manager.get_next_pending_task_for_job(job_id)

                # --- Check for Job Completion or Waiting State ---
                if not next_task:
                    if self.task_manager.has_running_tasks(job_id):
                        # If no pending tasks but some are still running, wait
                        logger.infi(f"Job {job_id}: No pending tasks, but waiting for {len([t for t in self.task_manager.tasks.values() if t.job_id == job_id and t.status == TaskStatus.RUNNING])} running tasks.")
                        await asyncio.sleep(1) # Wait a bit before checking again
                        continue
                    else:
                        # No pending and no running tasks left
                        logger.info(f"Job {job_id}: No more pending or running tasks.")
                        if not self.task_manager.has_errored_tasks(job_id):
                             logger.info(f"Job {job_id} completed successfully.")
                             job_final_status = JobStatus.COMPLETED
                             await self.broadcast_job_status(job_id, JobStatus.COMPLETED, "All tasks completed successfully.")
                        else:
                             logger.warning(f"Job {job_id} finished, but with errors or skipped tasks.")
                             job_final_status = JobStatus.FAILED # Mark as failed if errors occurred
                             await self.broadcast_job_status(job_id, JobStatus.FAILED, "Job finished, but some tasks failed or were skipped.")

                        # --- Final Report Saving/Broadcasting (Triggered on Completion/Failure) ---
                        logger.info(f"Job {job_id} finished with status {job_final_status}. Checking for final report content...")
                        await self._handle_final_report(job_id)
                        break # Exit the while loop, job is done

                # --- Execute the Next Task ---
                await self.task_manager.update_task_status(next_task.id, TaskStatus.RUNNING, detail="Orchestrator picked up task.")
                task_failed_in_dispatch = False
                try:
                    # Dispatch task to the appropriate agent via the dispatch map
                    await self._dispatch_task(next_task)

                    # If _dispatch_task completes *without raising an exception*,
                    # check the task's status. If it's still RUNNING, mark COMPLETED.
                    current_task_state = self.task_manager.get_task(next_task.id)
                    if current_task_state and current_task_state.status == TaskStatus.RUNNING:
                        await self.task_manager.update_task_status(
                            next_task.id,
                            TaskStatus.COMPLETED,
                            detail="Task completed successfully by agent."
                        )
                    elif current_task_state:
                         logger.debug(f"Task {next_task.id} finished with status {current_task_state.status} (not RUNNING). Not marking completed by Orchestrator.")
                    else:
                         # Should not happen if task existed before dispatch
                         logger.warning(f"Task {next_task.id} not found after dispatch. Cannot update status.")

                except Exception as e:
                    # Handle errors *raised by the agent execution* (_dispatch_task re-raises them)
                    task_failed_in_dispatch = True
                    logger.error(f"Error executing Task {next_task.id} (Type: {next_task.task_type.value}): {e}", exc_info=False) # Log simply here
                    error_msg = str(e)
                    # Ensure task exists before updating status to ERROR
                    if self.task_manager.get_task(next_task.id):
                        await self.task_manager.update_task_status(
                            next_task.id,
                            TaskStatus.ERROR,
                            error_message=error_msg,
                            detail=f"Task failed during execution: {error_msg[:100]}..."
                        )
                    else:
                        logger.error(f"Task {next_task.id} not found, cannot mark as ERROR after execution failure.")

                    # Pause the job because a task failed
                    #self.pause_events[job_id].set() # Set the event to signal pause
                    #await self.broadcast_job_status(job_id, JobStatus.PAUSED, f"Paused due to error in task {next_task.id}: {error_msg[:100]}...")
                    # Loop will pause on the next iteration due to the pause_event being set

                # --- Loop Delay ---
                # Add a small delay unless a task just failed (to avoid immediate re-check after pause)
                if not task_failed_in_dispatch:
                    await asyncio.sleep(0.1)
        except asyncio.CancelledError:
             logger.info(f"Job {job_id} task was cancelled.")
             job_final_status = JobStatus.FAILED
             await self.broadcast_job_status(job_id, JobStatus.FAILED, "Job execution was cancelled.")
        except Exception as e:
            logger.exception(f"Critical unexpected error in research flow for job {job_id}: {e}")
            job_final_status = JobStatus.FAILED
            await self.broadcast_job_status(job_id, JobStatus.FAILED, f"Critical error occurred: {e}")
        finally:
            # Final actions regardless of how the loop exited
            self.active_jobs[job_id] = job_final_status # Update final status
            logger.info(f"Exiting research flow loop for job {job_id}. Final Status: {job_final_status.value}")
            # Clean up resources for this job
            self.pause_events.pop(job_id, None)
            self.job_tasks.pop(job_id, None)
            logger.info(f"Cleaned up resources for job {job_id}.")

    async def _dispatch_task(self, task: Task):
        """
        Looks up the appropriate agent method based on TaskType and executes it,
        passing necessary parameters. Handles cases where agents are unavailable.
        Raises exceptions if agent execution fails.
        """
        logger.info(f"Dispatching Task Type: {task.task_type.value}, ID: {task.id}, Job: {task.job_id}")
        logger.info(f"Task parameters: {json.dumps(task.parameters)}")

        agent_method = self.agent_dispatch.get(task.task_type)

        if not agent_method:
            # Handle PLAN tasks (shouldn't be dispatched by loop) and unknown types
            if task.task_type == TaskType.PLAN:
                 logger.warning(f"PLAN task {task.id} reached dispatch logic unexpectedly. Marking completed.")
                 await self.task_manager.update_task_status(task.id, TaskStatus.COMPLETED, detail="Planning completed (conceptual task).")
                 return # Don't dispatch
            else:
                 logger.error(f"No agent method configured or agent unavailable for task type: {task.task_type.value}. Skipping task {task.id}.")
                 await self.task_manager.update_task_status(task.id, TaskStatus.SKIPPED, detail=f"No handler for task type {task.task_type.value}")
                 return # Skip if no handler

        # Prepare arguments for the agent method
        # Standard args: task_id, job_id
        # Specific args come from task.parameters
        kwargs_for_agent = task.parameters.copy() # Start with parameters dict
        kwargs_for_agent['task_id'] = task.id
        kwargs_for_agent['job_id'] = task.job_id

        try:
            # Call the appropriate agent method (e.g., self.search_agent.run(**kwargs_for_agent))
            # The agent method is responsible for its own logic and storing results via TaskManager
            await agent_method(**kwargs_for_agent)
            logger.info(f"Agent method for task {task.id} (Type: {task.task_type.value}) completed execution.")
        except Exception as agent_exec_error:
             # Log the specific error from the agent call
             logger.error(f"Agent execution failed for task {task.id} (Type: {task.task_type.value}). Error: {agent_exec_error}", exc_info=True)
             # Re-raise the exception so the main loop (_run_research_flow) catches it,
             # logs it again (with less detail maybe), sets task status to ERROR, and pauses the job.
             raise agent_exec_error
    async def _run_filter_agent(self, task_id: str, job_id: str, **kwargs):
         """Wrapper to find inputs (previous search task IDs) for the filter agent."""
         logger.info(f"Filter wrapper called for task {task_id}, job {job_id}. Kwargs ignored: {kwargs}")
         # Filter agent needs IDs of preceding *completed* SEARCH tasks
        
         # Get completed search tasks for this job  
         search_tasks = self.task_manager.get_completed_tasks_for_job(job_id, task_type=TaskType.SEARCH)
         search_task_ids = [st.id for st in search_tasks]
         
         if not search_task_ids:
              logger.warning(f"[Job {job_id} | Task {task_id}] Filter task found no preceding completed Search tasks.")
         else:
              logger.info(f"[Job {job_id} | Task {task_id}] Filter task using results from {len(search_tasks)} completed search tasks with IDs: {search_task_ids}")

         # Call the actual filter agent run method
         await self.filtering_agent.run(search_task_ids=search_task_ids, current_task_id=task_id, job_id=job_id)

         # Broadcast summary after filter completes successfully 
         filter_result = self.task_manager.get_result(task_id)
         if filter_result and isinstance(filter_result, dict):
             filtered_items = filter_result.get("filtered_results", [])
             summary = JobResultsSummary(
                     job_id=job_id,
                     unique_sources_found=len(filtered_items),
                     duplicates_removed=filter_result.get("duplicates_removed", 0),
                     sources_analyzed_count=filter_result.get("sources_analyzed_count", len(search_tasks)),
                     top_sources=[
                         {"title": item.get("title", "N/A"), "url": item.get("url", "N/A")}
                         for item in filtered_items[:5] if isinstance(item, dict)
                     ]
                 )
             await self.broadcast_job_summary(summary)


    async def _run_analysis_agent(self, task_id: str, job_id: str, topic: str, **kwargs):
         """Wrapper to find inputs (preceding filter task ID) for the analysis agent."""
         logger.debug(f"Analysis wrapper called for task {task_id}, job {job_id}. Topic: {topic}. Kwargs ignored: {kwargs}")
         # Analysis agent needs the filter results from the preceding filter task
         
         # Get the completed filter tasks for this job
         filter_tasks = self.task_manager.get_completed_tasks_for_job(job_id, task_type=TaskType.FILTER)
         
         if not filter_tasks:
             logger.error(f"[Job {job_id} | Task {task_id}] Cannot run Analysis: No preceding completed Filter task found.")
             # Raise error to stop processing this task
             raise ValueError("Preceding Filter task not found or not completed for Analysis.")
         
         # Assume latest completed filter task is the relevant one
         filter_task = filter_tasks[-1]  # Last one is most recent
         filter_task_id = filter_task.id
         
         # Get the actual filter result
         filter_result = self.task_manager.get_result(filter_task_id)
         if not filter_result:
             logger.error(f"[Job {job_id} | Task {task_id}] Filter task {filter_task_id} has no stored result.")
             raise ValueError(f"Filter task {filter_task_id} has no result for Analysis.")
             
         logger.info(f"[Job {job_id} | Task {task_id}] Analysis task using results from Filter Task {filter_task_id}")

         # Call the actual analysis agent run method with the filter result
         await self.analysis_agent.run(
             topic=topic, # Passed from parameters via kwargs_for_agent -> **kwargs
             filter_result=filter_result,  # Pass filter result directly
             current_task_id=task_id,
             job_id=job_id
         )


    async def _run_report_task(self, task_id: str, job_id: str, source_task_id: Optional[str] = None, **kwargs):
         """
         Handles the REPORT task. Typically finds the result of the preceding
         SYNTHESIZE or REASON task and stores it under the REPORT task's ID.
         If no such task is available, will try to use filter results directly.
         """
         logger.debug(f"Report wrapper called for task {task_id}, job {job_id}. Source: {source_task_id}. Kwargs ignored: {kwargs}")
         report_content = None
         
         if source_task_id:
             # Use specified source if provided
             result_to_report = self.task_manager.get_result(source_task_id)
             if result_to_report is None:
                 logger.warning(f"Report task {task_id} could not find result for specified source task {source_task_id}.")
                 # Continue to fallback methods
             else:
                 # Extract content from the specified source
                 if isinstance(result_to_report, dict):
                     report_content = result_to_report.get("report") or result_to_report.get("analysis_output") or result_to_report.get("reasoning_output")
                 elif isinstance(result_to_report, str):
                     report_content = result_to_report # Assume direct string output is the report
         
         # If no report content yet, try to find it from completed tasks
         if not report_content:
             # Find latest completed synthesis or reasoning task
             # Get all task objects from task_manager (not results!)
             tasks = list(self.task_manager.tasks.values())
             synthesis_tasks = []
             for task in reversed(tasks):
                 if task.job_id == job_id and task.status == TaskStatus.COMPLETED:
                     if task.task_type in [TaskType.SYNTHESIZE, TaskType.REASON]:
                         synthesis_tasks.append(task)
                         
             if synthesis_tasks:
                 # Use the latest completed task
                 latest_task = synthesis_tasks[0]
                 source_task_id = latest_task.id
                 logger.info(f"Report task {task_id} automatically targeting result from task {source_task_id} (Type: {latest_task.task_type.value})")
                 result_to_report = self.task_manager.get_result(source_task_id)
                 
                 if result_to_report:
                     # Extract content
                     if isinstance(result_to_report, dict):
                         report_content = result_to_report.get("report") or result_to_report.get("analysis_output") or result_to_report.get("reasoning_output")
                     elif isinstance(result_to_report, str):
                         report_content = result_to_report
             
         # Fallback: if still no report content, try using filter results directly
         if not report_content:
             logger.warning(f"Report task {task_id} could not find a relevant source task. Trying to use filter results as fallback.")
             
             # Get the latest filter task results
             filter_tasks = [t for t in self.task_manager.tasks.values() 
                            if t.job_id == job_id and t.status == TaskStatus.COMPLETED and t.task_type == TaskType.FILTER]
             
             if filter_tasks:
                 filter_task = filter_tasks[-1]  # Use the latest one
                 filter_result = self.task_manager.get_result(filter_task.id)
                 
                 if filter_result and isinstance(filter_result, dict) and "filtered_results" in filter_result:
                     # Generate a simple report from filter results
                     filtered_items = filter_result.get("filtered_results", [])
                     if filtered_items:
                         logger.info(f"Generating a simple report from {len(filtered_items)} filtered items as fallback.")
                         
                         # Create a simple markdown report listing the sources
                         report_lines = [
                             "# AI Impact on Software Development - Information Sources",
                             "",
                             "The following sources contain information about the impact of AI on software development jobs:",
                             ""
                         ]
                         
                         # Add the sources
                         for i, item in enumerate(filtered_items[:10], 1):  # Top 10 sources
                             title = item.get("title", "Untitled Source")
                             url = item.get("url", "No URL")
                             report_lines.append(f"{i}. [{title}]({url})")
                         
                         report_lines.extend([
                             "",
                             "## Note",
                             "",
                             "This is a simplified report listing sources only. The detailed synthesis could not be completed successfully."
                         ])
                         
                         report_content = "\n".join(report_lines)
         
         # If we have report content, store it
         if report_content and isinstance(report_content, str):
             # Store the extracted or generated content
             await self.task_manager.store_result(task_id, {"report": report_content})
             logger.info(f"Report task {task_id} completed with content from {source_task_id if source_task_id else 'fallback mechanism'}.")
             return
         
         # If we reached here, we couldn't generate any report
         logger.warning(f"Report task {task_id}: Could not extract or generate any report content.")
         await self.task_manager.store_result(task_id, {"report": "No report could be generated due to missing or invalid source data."})
         # Don't raise an exception - just store a placeholder report


    # --- Final Report Handling (Called at end of job) ---
    async def _handle_final_report(self, job_id: str):
        """Finds the final report, saves it, and broadcasts it."""
        logger.info(f"Job {job_id}: Checking for final report content...")
        
        # First try to find completed REPORT tasks
        report_task = None
        synthesis_task = None
        reason_task = None
        
        # Get all tasks from task_manager (not results!)
        tasks = list(self.task_manager.tasks.values())
        
        # Check tasks in reverse order for the most recent relevant one
        for task in reversed(tasks):
            if task.job_id == job_id and task.status == TaskStatus.COMPLETED:
                if task.task_type == TaskType.REPORT and not report_task:
                    report_task = task
                elif task.task_type == TaskType.SYNTHESIZE and not synthesis_task:
                    synthesis_task = task
                elif task.task_type == TaskType.REASON and not reason_task:
                    reason_task = task

        # Use the highest priority task found (REPORT > SYNTHESIZE > REASON)
        last_relevant_task = report_task or synthesis_task or reason_task

        if last_relevant_task:
             final_result = self.task_manager.get_result(last_relevant_task.id)
             report_markdown = None
             if isinstance(final_result, dict):
                 # Look for common keys
                 report_markdown = final_result.get("report") or final_result.get("analysis_output") or final_result.get("reasoning_output")
             elif isinstance(final_result, str):
                 report_markdown = final_result # Simple string result

             if isinstance(report_markdown, str) and report_markdown.strip():
                 logger.info(f"Found final report content in task {last_relevant_task.id} (Type: {last_relevant_task.task_type.value}). Saving and broadcasting.")
                 await self._save_report_to_file(job_id, report_markdown)
                 await self._broadcast_final_report(job_id, report_markdown)
             else:
                 logger.warning(f"Job {job_id} finished, but final report from task {last_relevant_task.id} was empty or invalid.")
        else:
             logger.warning(f"Job {job_id} finished, but no final REPORT, SYNTHESIZE, or REASON task found with results.")

    
    async def _dispatch_task_old_way(self, task: Task):
        """Calls the appropriate agent based on task description. Agents now store their own results."""
        logger.info(f"Dispatching task: {task.description} (ID: {task.id}) for Job {task.job_id}")
        desc_lower = task.description.lower()
        task_completed_successfully = False # Flag to track if agent ran without error
        logger.info(f"*******Task description lowercased: {desc_lower}")
        try:
            #loophole in below conditions - if the task has name "Search" then it will never enter filter and same when plan research is there it will never enter other else conditoins
            #TODO - fix this loophole
            if "plan research" in desc_lower:
                #TODO: next iteration try to seperate out planning from this tasks as it should ideally be once or at max 3 times in a job
                topic = task.description.split(":")[-1].strip()
                sub_task_descriptions = await self.planning_agent.run(topic=topic, job_id=task.job_id)
                await self._handle_planning_completion(task, sub_task_descriptions) #only to add anaalyse and generate report tasks
                await self.task_manager.store_result(task.id, sub_task_descriptions)
                task_completed_successfully = True

            elif "search:" in desc_lower:
                await asyncio.sleep(SEARCH_TASK_DELAY)  # Simulate delay for search tasks
                query = task.description.split(":")[-1].strip()
                await self.search_agent.run(query=query, task_id=task.id, job_id=task.job_id)
                task_completed_successfully = True
                # Result stored by search_agent internally

            elif "filter:" in desc_lower:
                # Identify preceding search tasks for this job
                search_tasks = self.task_manager.get_completed_tasks_for_job(task.job_id, description_contains="Search:")
                search_task_ids = [st.id for st in search_tasks]

                if not search_task_ids:
                    logger.warning(f"[Job {task.job_id} | Task {task.id}] Filter task found no preceding completed Search tasks.")
                    # Run filter even with empty list to potentially store an empty result
                    await self.filtering_agent.run(search_task_ids=[], current_task_id=task.id, job_id=task.job_id)
                else:
                    logger.info(f"[Job {task.job_id} | Task {task.id}] Filter task will use results from search tasks: {search_task_ids}")
                    await self.filtering_agent.run(search_task_ids=search_task_ids, current_task_id=task.id, job_id=task.job_id)

                # --- START: Add Summary Broadcast ---
                # Check if filter completed and get its result
                filter_result = self.task_manager.get_result(task.id)
                if filter_result and isinstance(filter_result, dict):
                    #TODO: advanced version is mimking how grok/perplexity display top ranking one directly in UI , To make response more effective here, need to figure out ranking pages effectively ( may be another async coroutine that will rank as and when a page lands in graph)
                    filtered_items = filter_result.get("filtered_results", [])
                    summary = JobResultsSummary(
                        job_id=task.job_id,
                        unique_sources_found=len(filtered_items),
                        duplicates_removed=filter_result.get("duplicates_removed", 0),
                        sources_analyzed_count=filter_result.get("sources_analyzed_count", len(search_task_ids)),
                        # Get top 5 URLs/titles (if available)
                        top_sources=[
                            {"title": item.get("title", "N/A"), "url": item.get("url", "N/A")}
                            for item in filtered_items[:5] if isinstance(item, dict)
                        ]
                    )
                    await self.broadcast_job_summary(summary)
                    task_completed_successfully = True # Mark as successful if result was processed
                else:
                     logger.warning(f"[Job {task.job_id} | Task {task.id}] Filter task completed, but no valid result dictionary found in TaskManager.")
                     # Still mark as successful if the agent didn't raise an error, even if result missing
                     task_completed_successfully = True
                # --- END: Add Summary Broadcast ---

            elif "analyze" in desc_lower or "synthesize" in desc_lower:
                logger.info(f"*************Running Analysis Agent******************")
                if not self.analysis_agent:
                     logger.error(f"[Job {task.job_id} | Task {task.id}] Analysis Agent not initialized (LLM provider failed?). Skipping task.")
                     await self.task_manager.update_task_status(task.id, TaskStatus.SKIPPED, detail="Analysis Agent unavailable.")
                     task_completed_successfully = False # Explicitly skipped
                else:
                     # Find the preceding filter task for this job
                     filter_tasks = self.task_manager.get_completed_tasks_for_job(task.job_id, description_contains="Filter:")
                     logger.info(f"****************Found {len(filter_tasks)} completed Filter tasks for this job.")
                     if not filter_tasks:
                          logger.error(f"[Job {task.job_id} | Task {task.id}] Cannot run Analysis: No preceding completed Filter task found.")
                          await self.task_manager.update_task_status(task.id, TaskStatus.ERROR, error_message="Preceding Filter task not found or not completed.", detail="Failed: Missing input from Filter task.")
                          task_completed_successfully = False # Error state
                     else:
                          # Assuming the latest filter task is the relevant one
                          filter_task_id = filter_tasks[-1].id
                          #TODO: enhance below topic extraction, this might miss users actual question
                          topic = self.task_manager.get_task(task.job_id).description.split(":")[-1].strip() # Get topic from initial planning task
                          logger.info(f"[Job {task.job_id} | Task {task.id}] Running Analysis Agent using results from Filter Task {filter_task_id}")

                          await self.analysis_agent.run(
                              topic=topic,
                              filter_task_id=filter_task_id,
                              current_task_id=task.id,
                              job_id=task.job_id
                          )
                          task_completed_successfully = True
                          # Result stored by analysis_agent internally

            elif "generate report" in desc_lower:
                logger.info(f"[Job {task.job_id} | Task {task.id}] Running Report Generation (Not Implemented Yet - Analysis agent produces the report)")
                # Check if analysis task completed and get its result
                analysis_tasks = self.task_manager.get_completed_tasks_for_job(task.job_id, description_contains="Analyze")
                if analysis_tasks:
                     analysis_result = self.task_manager.get_result(analysis_tasks[-1].id)
                     if analysis_result and isinstance(analysis_result, dict) and "report" in analysis_result:
                          logger.info(f"[Job {task.job_id} | Task {task.id}] Found report generated by Analysis task.")
                          # Store the same report under the report task ID for clarity/provenance
                          await self.task_manager.store_result(task.id, analysis_result)
                          task_completed_successfully = True
                     else:
                          logger.warning(f"[Job {task.job_id} | Task {task.id}] Analysis task completed but no report found in its result.")
                          await self.task_manager.update_task_status(task.id, TaskStatus.SKIPPED, detail="Input report from Analysis task missing.")
                          task_completed_successfully = False
                else:
                     logger.warning(f"[Job {task.job_id} | Task {task.id}] Cannot generate report: Preceding Analysis task not found or not completed.")
                     await self.task_manager.update_task_status(task.id, TaskStatus.SKIPPED, detail="Preceding Analysis task missing.")
                     task_completed_successfully = False
            else:
                logger.warning(f"No specific agent logic found for task: '{task.description}'. Marking as skipped.")
                await self.task_manager.update_task_status(task.id, TaskStatus.SKIPPED, detail="No agent handler found for this task type.")
                # Task was skipped, not successfully completed in the normal sense
                task_completed_successfully = False # Explicitly false

            # If the agent ran without raising an exception, mark task completed *if* it wasn't already skipped/errored
            if task_completed_successfully:
                current_task_state = self.task_manager.get_task(task.id)
                if current_task_state and current_task_state.status == TaskStatus.RUNNING:
                    await self.task_manager.update_task_status(
                        task.id,
                        TaskStatus.COMPLETED,
                        detail="Task completed successfully by agent."
                    )
                elif current_task_state:
                     logger.debug(f"Task {task.id} status is already {current_task_state.status}, not marking completed again.")
                else:
                     logger.warning(f"Task {task.id} not found after successful agent run. Cannot mark completed.")


        except Exception as e:
            # Handle errors raised *by the agent execution*
            logger.error(f"Agent execution failed for task {task.id} ({task.description}): {e}", exc_info=True) # Log traceback here
            error_msg = str(e)
            # Ensure task exists before updating status to ERROR
            if self.task_manager.get_task(task.id):
                await self.task_manager.update_task_status(
                    task.id,
                    TaskStatus.ERROR,
                    error_message=error_msg,
                    detail=f"Agent failed: {error_msg[:100]}..."
                )
            else:
                logger.error(f"Task {task.id} not found, cannot mark as ERROR after agent failure.")

            # Pause the job - Moved this responsibility to the main loop (_run_research_flow)
            # self.pause_events[task.job_id].set()
            # await self.broadcast_job_status(task.job_id, JobStatus.PAUSED, f"Paused due to error in task {task.id}: {error_msg[:100]}...")

            # Re-raise the exception so the main loop catches it and pauses
            raise e
    async def _handle_planning_completion(self, planning_task: Task, sub_task_descriptions: List[str]):
        if not isinstance(sub_task_descriptions, list):
             logger.warning(f"Planner for task {planning_task.id} did not return a list of descriptions. Got: {type(sub_task_descriptions)}")
             # Store empty list as result to indicate failure/no output
             await self.task_manager.store_result(planning_task.id, [])
             return

        logger.info(f"Planning task {planning_task.id} completed. Adding {len(sub_task_descriptions)} sub-tasks for job {planning_task.job_id}.")
        for sub_task_desc in sub_task_descriptions:
            await self.task_manager.add_task(description=sub_task_desc, job_id=planning_task.job_id)

        # Add downstream tasks only if planning was successful
        has_analysis = any("analyze" in d.lower() or "synthesize" in d.lower() for d in sub_task_descriptions)
        has_report = any("generate report" in d.lower() for d in sub_task_descriptions)

        # Check existing tasks to avoid adding duplicates if run multiple times
        existing_tasks = self.task_manager.get_all_tasks()
        #TODO: Loophole here as well since we directly check strings in description which may not be always accurate and we might miss analysing for few edge cases, rethink on strategy
        analysis_exists = any(("analyze" in t.description.lower() or "synthesize" in t.description.lower()) and t.job_id == planning_task.job_id for t in existing_tasks)
        report_exists = any(("generate report" in t.description.lower()) and t.job_id == planning_task.job_id for t in existing_tasks)

        if not has_analysis and not analysis_exists:
             await self.task_manager.add_task(description=f"Analyze & Synthesize findings for job {planning_task.job_id}", job_id=planning_task.job_id)
        if not has_report and not report_exists:
             await self.task_manager.add_task(description=f"Generate report for job {planning_task.job_id}", job_id=planning_task.job_id)

        # Store the generated sub-task descriptions as the result of the planning task
        # Moved this here from _dispatch_task to ensure it happens after adding tasks
        # await self.task_manager.store_result(planning_task.id, sub_task_descriptions)
        # Storing result is now done in _dispatch_task right after planning_agent.run

    async def resume(self, job_id: str):
        """Resumes a paused job."""
        if job_id in self.pause_events and self.pause_events[job_id].is_set():
             logger.info(f"Received resume command for paused job {job_id}.")
             self.active_jobs[job_id] = JobStatus.RUNNING # Ensure status is RUNNING
             self.pause_events[job_id].clear() # Clear the event to allow the loop to continue
             # Don't broadcast here, the loop will broadcast upon resuming
        else:
             logger.warning(f"Received resume command for job {job_id}, but it was not paused or doesn't exist.")

    async def skip_task_and_resume(self, job_id: str, task_id: str):
         """Marks a task as skipped and potentially resumes the job."""
         logger.info(f"Received skip command for task {task_id} in job {job_id}.")
         await self.task_manager.update_task_status(task_id, TaskStatus.SKIPPED, detail="Task skipped by user.")
         # If the job was paused specifically because *this task* failed, resume it.
         if job_id in self.pause_events and self.pause_events[job_id].is_set():
              # Check if the paused state was due to this task (may need better state tracking)
              # Simple assumption: if paused, skipping an error task should allow resume
              logger.info(f"Job {job_id} was paused, resuming after skipping task {task_id}.")
              await self.resume(job_id)


    async def broadcast_job_status(self, job_id: str, status: JobStatus, detail: str):
        """Broadcasts overall job status updates via WebSocket."""
        message = {
            "type": "job_status",
            "job_id": job_id,
            "status": status.value,
            "detail": detail
        }
        await self.task_manager.connection_manager.broadcast_json(message) # Use TM's connection mgr

    async def broadcast_job_summary(self, summary: JobResultsSummary):
        """Broadcasts the research results summary via WebSocket."""
        logger.info(f"Broadcasting results summary for Job {summary.job_id}: Found {summary.unique_sources_found} sources.")
        # Use TaskManager's connection_manager instance
        await self.task_manager.connection_manager.broadcast_json(summary.dict())

    async def _save_report_to_file(self, job_id: str, report_markdown: str):
        """Saves the final report markdown to a file."""
        try:
            # Create output directory if it doesn't exist
            os.makedirs(OUTPUT_DIR, exist_ok=True)

            # Sanitize job_id for filename (optional, depends on job_id format)
            safe_job_id = job_id.replace(":", "_").replace("/", "_").replace("\\", "_")
            filename = os.path.join(OUTPUT_DIR, f"{safe_job_id}_report.md")

            with open(filename, "w", encoding="utf-8") as f:
                f.write(report_markdown)
            logger.info(f"Successfully saved report for job {job_id} to {filename}")

        except IOError as e:
            logger.error(f"Failed to save report file for job {job_id}: {e}")
        except Exception as e:
            logger.exception(f"An unexpected error occurred while saving report for job {job_id}: {e}")

    async def _broadcast_final_report(self, job_id: str, report_markdown: str):
        """Broadcasts the final report via WebSocket."""
        logger.info(f"Broadcasting final report for job {job_id} via WebSocket.")
        message = FinalReportMessage(job_id=job_id, report_markdown=report_markdown)
        # Use TaskManager's connection_manager instance
        await self.task_manager.connection_manager.broadcast_json(message.dict())
    


    async def broadcast_job_summary(self, summary: JobResultsSummary):
         """Broadcasts the research results summary via WebSocket."""
         logger.info(f"Broadcasting results summary for Job {summary.job_id}: Found {summary.unique_sources_found} sources.")
         await self.task_manager.connection_manager.broadcast_json(summary.dict())

    async def _handle_report_completion(self, task: Task):
        """
        Checks if a completed task is the final report/analysis task,
        and if so, retrieves, saves, and broadcasts the report.
        """
        desc_lower = task.description.lower()
        job_id = task.job_id # Get job_id from the task object

        # Check if this task type is expected to produce the final report
        if "analyze" in desc_lower or "synthesize" in desc_lower or "generate report" in desc_lower:
            logger.info(f"[Job {job_id}] Task {task.id} ({task.description}) completed. Checking for final report...")

            analysis_result = self.task_manager.get_result(task.id)
            logger.debug(f"[Job {job_id}] Result retrieved for task {task.id}: {analysis_result is not None}") # Log if result exists

            if analysis_result and isinstance(analysis_result, dict) and "report" in analysis_result:
                report_markdown = analysis_result["report"]
                if isinstance(report_markdown, str) and report_markdown.strip():
                    logger.info(f"[Job {job_id}] Valid report found for task {task.id}. Saving and broadcasting.")
                    # Perform saving and broadcasting
                    await self._save_report_to_file(job_id, report_markdown)
                    await self._broadcast_final_report(job_id, report_markdown)
                else:
                    logger.warning(f"[Job {job_id}] Report found for task {task.id}, but it's empty or not a string.")
            else:
                logger.warning(f"[Job {job_id}] Analysis/Report task {task.id} completed, but no valid 'report' key found in its result.")
        # else:
            # logger.debug(f"Task {task.id} ({task.description}) is not a report-generating task. Skipping report handling.")